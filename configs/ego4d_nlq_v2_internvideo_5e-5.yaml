dataset_name: ego4d
devices: cuda:0
train_split: ['training']
val_split: ['validation']
dataset: {
  json_file: ./ego4d_data/ego4d_nlq_v2_ori_data/nlq_val.json,
  train_jsonl_file: ./ego4d_data/ego4d_nlq_train_v2.jsonl,
  val_jsonl_file: ./ego4d_data/ego4d_nlq_val_v2.jsonl,
  video_feat_dir: /s1_md0/leiji/v-zhijian/ego4d_data_2023/ego4d_lmdb/em_egovlp+internvideo_visual_features_1.87fps,
  text_feat_dir: /s1_md0/leiji/v-zhijian/ego4d_nlq_cvpr_2023_data/offline_lmdb/nlq_v2_clip_token_features,
  val_text_feat_dir: /s1_md0/leiji/v-zhijian/ego4d_nlq_cvpr_2023_data/offline_lmdb/nlq_v2_clip_token_features,
  num_classes: 1,
  input_vid_dim: 2304,
  input_txt_dim: 512,
  feat_stride: 16.043,
  num_frames: 16.043,
  default_fps: 30,
  trunc_thresh: 0.3 ,
  crop_ratio: [0.9, 1.0],
  max_seq_len: 2560,
}
model: {
  fpn_type: identity,
  max_buffer_len_factor: 4.0,
  n_mha_win_size: 9,
  backbone_arch: [2, 4, 4, 0, 6],
  # shrink the model for reduced input feature channels
  n_head: 4,
  embd_dim: 384,
  fpn_dim: 384,
  head_dim: 384,
  use_abs_pe: True,
  regression_range: [[0, 4], [2, 8], [4, 16], [8, 32], [16, 64], [32, 128],[64, 10000]],
}
opt: {
  learning_rate: 0.00005,
  backbone_lr_weight: 1,
  epochs: 8,
  warmup_epochs: 5,
  weight_decay: 0.05,
}
loader: {
  batch_size: 2,
}
train_cfg: {
  init_loss_norm: 200,
  clip_grad_l2norm: 1.0,
  cls_prior_prob: 0.01,
  center_sample: radius,
  center_sample_radius: 1.5,
  label_smoothing: 0.1,
  droppath: 0.1,
  loss_weight: 1.0,
}
test_cfg: {
  voting_thresh: 0.9,
  pre_nms_topk: 2000,
  # max of 50 predictions per video
  max_seg_num: 5,
  min_score: 0.001,
  nms_sigma : 0.75,
  duration_thresh: 0.001,
}
output_folder: ./ckpt/
